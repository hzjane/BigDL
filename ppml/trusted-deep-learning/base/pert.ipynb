{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23e69a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from torch import nn\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05bf9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distributed conf\n",
    "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "RANK = int(os.environ.get(\"RANK\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "053bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    print(123)\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch PERT Example\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16, metavar=\"N\",\n",
    "                        help=\"input batch size for training (default: 16)\")\n",
    "    parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
    "                        help=\"input batch size for testing (default: 1000)\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, metavar=\"N\",\n",
    "                        help=\"number of epochs to train (default: 10)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-5, metavar=\"LR\",\n",
    "                        help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
    "                        help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\"--dataset\", type=int, default=1, metavar=\"D\",\n",
    "                        help=\"dataset size (default 1 * 9600)\")\n",
    "    parser.add_argument(\"--save-model\", action=\"store_true\", default=False,\n",
    "                        help=\"For Saving the current Model\")\n",
    "    parser.add_argument(\"--local-only\", action=\"store_true\", default=False,\n",
    "                        help=\"If set to true, then load model from disk\")\n",
    "    parser.add_argument(\"--model-path\", type=str, default=\"/ppml/model\",\n",
    "                        help=\"Where to load model\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\",\n",
    "                    help=\"Where to train model, default is cpu\")\n",
    "    # Only for test purpose\n",
    "    parser.add_argument(\"--load-model\", action=\"store_true\", default=False,\n",
    "                        help=\"For loading the current model\")\n",
    "    parser.add_argument(\"--mini-batch\", type=int, default=0, metavar=\"M\",\n",
    "                    help=\"If set, the PyTorch will conduct M local-batch computation before doing a all_reduce sync\")\n",
    "    parser.add_argument(\"--log-interval\", type=int, default=2, metavar=\"N\",\n",
    "                    help=\"how many batches to wait before logging training status\")\n",
    "    parser.add_argument(\"--log-path\", type=str, default=\"\",\n",
    "                    help=\"Path to save logs. Print to StdOut if log-path is not set\")\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71182fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_log_path(args):\n",
    "    print(456)\n",
    "    if args.log_path == \"\":\n",
    "        logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        level=logging.DEBUG)\n",
    "    else:\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            level=logging.DEBUG,\n",
    "            filename=args.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce65545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # data_type is actually split, so that we can define dataset for train set/validate set\n",
    "    def __init__(self, data_type, dataset_load):\n",
    "        self.dataset_load = dataset_load\n",
    "        self.data = self.load_data(data_type)\n",
    "\n",
    "    def load_data(self, data_type):\n",
    "        tmp_dataset = load_dataset(path='seamew/ChnSentiCorp', split=data_type)\n",
    "        Data = {}\n",
    "        # So enumerate will return a index, and  the line?\n",
    "        # line is a dict, including 'text', 'label'\n",
    "#         if data_type == 'train':\n",
    "        for i in range(self.dataset_load):\n",
    "            for idx, line in enumerate(tmp_dataset):\n",
    "                sample = line\n",
    "                Data[idx + i * len(tmp_dataset)] = sample\n",
    "#         else:\n",
    "#             for idx, line in enumerate(tmp_dataset):\n",
    "#                 sample = line\n",
    "#                 Data[idx] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4fe5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(args, train_data, valid_data, tokenizer):\n",
    "    if is_distributed():\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True, drop_last=False, seed=args.seed)\n",
    "        valid_sampler = DistributedSampler(\n",
    "            valid_data, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True, drop_last=False, seed=args.seed)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, collate_fn=lambda x: collate_fn(x, tokenizer), sampler=train_sampler)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_data, batch_size=args.test_batch_size, collate_fn=lambda x: collate_fn(x, tokenizer), sampler=valid_sampler)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_data, batch_size=args.test_batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "    return train_dataloader, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2aa3cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_distribute():\n",
    "    return dist.is_available() and WORLD_SIZE > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ecbb221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_distributed():\n",
    "    return dist.is_available() and dist.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e761643",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Define dataset, so it is easier to load different split in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f4ca11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a batch of data, which is used for training\n",
    "def collate_fn(batch_samples, tokenizer):\n",
    "    batch_text = []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_text.append(sample['text'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    # The tokenizer will make the data to be a good format for our model to understand\n",
    "    X = tokenizer(\n",
    "        batch_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee8566cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        if args.local_only:\n",
    "            checkpoint = args.model_path\n",
    "            self.bert_encoder = BertModel.from_pretrained(\n",
    "                checkpoint, local_files_only=True)\n",
    "        else:\n",
    "            checkpoint = 'hfl/chinese-pert-base'\n",
    "            self.bert_encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "683bdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "def train_loop(args, dataloader, model, loss_fn, optimizer, epoch, total_loss):\n",
    "    # Set to train mode\n",
    "    model.train()\n",
    "    total_dataset = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    enumerator = enumerate(dataloader, start=1)\n",
    "    for batch, (X, y) in enumerator:\n",
    "        my_context = model.no_sync if WORLD_SIZE > 1 and args.mini_batch > 0 and batch % args.mini_batch != 0 else nullcontext\n",
    "        with my_context():\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            # Forward pass\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "        if args.mini_batch == 0 or batch % args.mini_batch == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_dataset += args.batch_size\n",
    "        from torch.utils.tensorboard import SummaryWriter   \n",
    "        writer = SummaryWriter('/ppml/test/pert.log')\n",
    "        writer.add_scalar('loss',loss.item(), (epoch - 1) * len(dataloader) + batch)\n",
    "        if batch % args.log_interval == 0:\n",
    "            msg = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
    "                epoch, batch, len(dataloader),\n",
    "                100. * batch / len(dataloader), loss.item())\n",
    "            logging.info(msg)\n",
    "\n",
    "    return total_loss, total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6aecfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test loop to get acc\n",
    "def test_loop(args, dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    #correct /= size\n",
    "    #correct *= WORLD_SIZE\n",
    "    correct = correct / (size / WORLD_SIZE)\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03c1ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(args, train_dataloader, valid_dataloader, model, loss_fn, optimizer):\n",
    "    total_loss = 0.\n",
    "    best_acc = 0.\n",
    "    total_time = 0.\n",
    "    total_throughput = 0.\n",
    "\n",
    "    for t in range(args.epochs):\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1}\\n-------------------------------\")\n",
    "        if is_distributed():\n",
    "            # set seed\n",
    "            train_dataloader.sampler.set_epoch(t)\n",
    "            valid_dataloader.sampler.set_epoch(t)\n",
    "        start = time.perf_counter()\n",
    "        # start to train\n",
    "        total_loss, total_dataset = train_loop(\n",
    "            args, train_dataloader, model, loss_fn, optimizer, t+1, total_loss)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1} Elapsed time:\",\n",
    "              end - start, flush=True)\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1} Processed dataset length:\",\n",
    "              total_dataset, flush=True)\n",
    "        msg = \"Epoch {}/{} Throughput: {: .4f}\".format(\n",
    "            t+1, args.epochs+1, 1.0 * total_dataset / (end-start))\n",
    "        total_time += (end - start)\n",
    "        total_throughput += total_dataset\n",
    "        print(msg, flush=True)\n",
    "        # to valid acc\n",
    "        valid_acc = test_loop(args, valid_dataloader, model, mode='Valid')\n",
    "\n",
    "    print(\"[INFO]Finish all test\", flush=True)\n",
    "    msg = \"[INFO]Average training time per epoch: {: .4f}\".format(total_time / args.epochs)\n",
    "    print(msg, flush=True)\n",
    "\n",
    "    msg = \"[INFO]Average throughput per epoch: {: .4f}\".format(total_throughput / total_time)\n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64ee63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=None):\n",
    "    # parse args\n",
    "    args = parse_args(args)\n",
    "    print(args)\n",
    "    # set log_path\n",
    "    set_log_path(args)\n",
    "    \n",
    "    # init tokenizer\n",
    "    if args.local_only:\n",
    "        checkpoint = args.model_path\n",
    "        tokenizer = BertTokenizer.from_pretrained(\n",
    "        checkpoint, model_max_length=512, local_files_only=True)\n",
    "    else:\n",
    "        checkpoint = 'hfl/chinese-pert-base'\n",
    "        tokenizer = BertTokenizer.from_pretrained(checkpoint, model_max_length=512)\n",
    "\n",
    "    torch.manual_seed(args.seed) # set seed\n",
    "    \n",
    "    # init pytorch distributed network if need\n",
    "    if should_distribute():\n",
    "        print(\"Using distributed PyTorch with {} backend\".format(\n",
    "            \"GLOO\"), flush=True)\n",
    "        dist.init_process_group(backend=dist.Backend.GLOO)\n",
    "\n",
    "    # Load train and valid data\n",
    "    print(\"[INFO]Before data get loaded\", flush=True)\n",
    "    train_data = Dataset('train', args.dataset)\n",
    "    print(\"######train data length:\", len(train_data.data), flush=True)\n",
    "    valid_data = Dataset('validation', 1)\n",
    "\n",
    "    # init data_loader\n",
    "    train_dataloader, valid_dataloader = get_dataloader(args, train_data, valid_data, tokenizer)\n",
    "    print(\"[INFO]Data get loaded successfully\", flush=True)\n",
    "\n",
    "    #init model\n",
    "    model = NeuralNetwork(args).to(args.device)\n",
    "    print(\"what happen\")\n",
    "    # load pre-train model\n",
    "    if (args.load_model):\n",
    "        model.load_state_dict(torch.load('./pert.bin'))\n",
    "    # local or distributed model\n",
    "    if is_distributed():\n",
    "        Distributor = nn.parallel.DistributedDataParallel\n",
    "        model = Distributor(model, find_unused_parameters=True)\n",
    "    \n",
    "    # set loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    # train epoch\n",
    "    save_model = do_train(args, train_dataloader, valid_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    # save model and exit\n",
    "    if (args.save_model):\n",
    "        torch.save(save_model.state_dict(), \"pert.bin\")\n",
    "    if is_distributed():\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7fdbf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "Namespace(batch_size=16, dataset=1, device='cpu', epochs=2, load_model=False, local_only=True, log_interval=20, log_path='', lr=1e-05, mini_batch=0, model_path='/ppml/model', save_model=False, seed=1, test_batch_size=16)\n",
      "456\n",
      "[INFO]Before data get loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16T01:47:10Z WARNING  Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/seamew--ChnSentiCorp/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85 (last modified on Fri Oct 28 00:22:19 2022) since it couldn't be found locally at seamew/ChnSentiCorp.\n",
      "2023-05-16T01:47:10Z DEBUG    open file: /root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/dataset_info.json\n",
      "2023-05-16T01:47:10Z WARNING  Found cached dataset chn_senti_corp (/root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n",
      "2023-05-16T01:47:10Z DEBUG    open file: /root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######train data length: 9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16T01:47:11Z WARNING  Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/seamew--ChnSentiCorp/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85 (last modified on Fri Oct 28 00:22:19 2022) since it couldn't be found locally at seamew/ChnSentiCorp.\n",
      "2023-05-16T01:47:11Z DEBUG    open file: /root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/dataset_info.json\n",
      "2023-05-16T01:47:11Z WARNING  Found cached dataset chn_senti_corp (/root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n",
      "2023-05-16T01:47:11Z DEBUG    open file: /root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Data get loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /ppml/model were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what happen\n",
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16T01:48:01Z INFO     Train Epoch: 1 [20/600 (3%)]\tloss=0.6981\n",
      "2023-05-16T01:48:41Z INFO     Train Epoch: 1 [40/600 (7%)]\tloss=0.6334\n",
      "2023-05-16T01:49:26Z INFO     Train Epoch: 1 [60/600 (10%)]\tloss=0.7035\n",
      "2023-05-16T01:50:03Z INFO     Train Epoch: 1 [80/600 (13%)]\tloss=0.3341\n",
      "2023-05-16T01:50:46Z INFO     Train Epoch: 1 [100/600 (17%)]\tloss=0.3500\n",
      "2023-05-16T01:51:45Z INFO     Train Epoch: 1 [120/600 (20%)]\tloss=0.3143\n",
      "2023-05-16T01:52:23Z INFO     Train Epoch: 1 [140/600 (23%)]\tloss=0.3626\n",
      "2023-05-16T01:53:01Z INFO     Train Epoch: 1 [160/600 (27%)]\tloss=0.2045\n",
      "2023-05-16T01:53:38Z INFO     Train Epoch: 1 [180/600 (30%)]\tloss=0.1844\n",
      "2023-05-16T01:54:17Z INFO     Train Epoch: 1 [200/600 (33%)]\tloss=0.1727\n",
      "2023-05-16T01:54:56Z INFO     Train Epoch: 1 [220/600 (37%)]\tloss=0.2468\n",
      "2023-05-16T01:55:48Z INFO     Train Epoch: 1 [240/600 (40%)]\tloss=0.5617\n",
      "2023-05-16T01:56:29Z INFO     Train Epoch: 1 [260/600 (43%)]\tloss=0.1602\n",
      "2023-05-16T01:57:09Z INFO     Train Epoch: 1 [280/600 (47%)]\tloss=0.1640\n",
      "2023-05-16T01:57:57Z INFO     Train Epoch: 1 [300/600 (50%)]\tloss=0.1606\n",
      "2023-05-16T01:58:45Z INFO     Train Epoch: 1 [320/600 (53%)]\tloss=0.0640\n",
      "2023-05-16T01:59:28Z INFO     Train Epoch: 1 [340/600 (57%)]\tloss=0.4117\n",
      "2023-05-16T02:00:17Z INFO     Train Epoch: 1 [360/600 (60%)]\tloss=0.2659\n",
      "2023-05-16T02:01:20Z INFO     Train Epoch: 1 [380/600 (63%)]\tloss=0.1630\n",
      "2023-05-16T02:02:04Z INFO     Train Epoch: 1 [400/600 (67%)]\tloss=0.2085\n",
      "2023-05-16T02:03:03Z INFO     Train Epoch: 1 [420/600 (70%)]\tloss=0.1153\n",
      "2023-05-16T02:03:58Z INFO     Train Epoch: 1 [440/600 (73%)]\tloss=0.0872\n",
      "2023-05-16T02:04:44Z INFO     Train Epoch: 1 [460/600 (77%)]\tloss=0.0989\n",
      "2023-05-16T02:05:39Z INFO     Train Epoch: 1 [480/600 (80%)]\tloss=0.0665\n",
      "2023-05-16T02:06:21Z INFO     Train Epoch: 1 [500/600 (83%)]\tloss=0.0525\n",
      "2023-05-16T02:07:05Z INFO     Train Epoch: 1 [520/600 (87%)]\tloss=0.0593\n",
      "2023-05-16T02:08:00Z INFO     Train Epoch: 1 [540/600 (90%)]\tloss=0.2021\n",
      "2023-05-16T02:08:31Z INFO     Train Epoch: 1 [560/600 (93%)]\tloss=0.0923\n",
      "2023-05-16T02:09:15Z INFO     Train Epoch: 1 [580/600 (97%)]\tloss=0.0989\n",
      "2023-05-16T02:10:06Z INFO     Train Epoch: 1 [600/600 (100%)]\tloss=0.1004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Elapsed time: 1373.5751556998584\n",
      "Epoch 1/3 Processed dataset length: 9600\n",
      "Epoch 1/3 Throughput:  6.9891\n",
      "Valid Accuracy: 94.8%\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16T02:15:16Z INFO     Train Epoch: 2 [20/600 (3%)]\tloss=0.0957\n",
      "2023-05-16T02:16:19Z INFO     Train Epoch: 2 [40/600 (7%)]\tloss=0.3168\n",
      "2023-05-16T02:16:57Z INFO     Train Epoch: 2 [60/600 (10%)]\tloss=0.0548\n",
      "2023-05-16T02:17:45Z INFO     Train Epoch: 2 [80/600 (13%)]\tloss=0.2561\n",
      "2023-05-16T02:18:42Z INFO     Train Epoch: 2 [100/600 (17%)]\tloss=0.0758\n",
      "2023-05-16T02:19:26Z INFO     Train Epoch: 2 [120/600 (20%)]\tloss=0.0508\n",
      "2023-05-16T02:20:15Z INFO     Train Epoch: 2 [140/600 (23%)]\tloss=0.3420\n",
      "2023-05-16T02:21:01Z INFO     Train Epoch: 2 [160/600 (27%)]\tloss=0.3407\n",
      "2023-05-16T02:21:50Z INFO     Train Epoch: 2 [180/600 (30%)]\tloss=0.0807\n",
      "2023-05-16T02:22:30Z INFO     Train Epoch: 2 [200/600 (33%)]\tloss=0.1635\n",
      "2023-05-16T02:23:15Z INFO     Train Epoch: 2 [220/600 (37%)]\tloss=0.0935\n",
      "2023-05-16T02:24:03Z INFO     Train Epoch: 2 [240/600 (40%)]\tloss=0.1969\n",
      "2023-05-16T02:24:45Z INFO     Train Epoch: 2 [260/600 (43%)]\tloss=0.0577\n",
      "2023-05-16T02:25:27Z INFO     Train Epoch: 2 [280/600 (47%)]\tloss=0.1261\n",
      "2023-05-16T02:26:20Z INFO     Train Epoch: 2 [300/600 (50%)]\tloss=0.1119\n",
      "2023-05-16T02:27:03Z INFO     Train Epoch: 2 [320/600 (53%)]\tloss=0.3043\n",
      "2023-05-16T02:28:07Z INFO     Train Epoch: 2 [340/600 (57%)]\tloss=0.3871\n",
      "2023-05-16T02:28:49Z INFO     Train Epoch: 2 [360/600 (60%)]\tloss=0.0592\n",
      "2023-05-16T02:29:34Z INFO     Train Epoch: 2 [380/600 (63%)]\tloss=0.2693\n",
      "2023-05-16T02:30:32Z INFO     Train Epoch: 2 [400/600 (67%)]\tloss=0.2746\n",
      "2023-05-16T02:31:21Z INFO     Train Epoch: 2 [420/600 (70%)]\tloss=0.1242\n",
      "2023-05-16T02:32:12Z INFO     Train Epoch: 2 [440/600 (73%)]\tloss=0.0508\n",
      "2023-05-16T02:32:56Z INFO     Train Epoch: 2 [460/600 (77%)]\tloss=0.0645\n",
      "2023-05-16T02:33:41Z INFO     Train Epoch: 2 [480/600 (80%)]\tloss=0.1402\n",
      "2023-05-16T02:34:35Z INFO     Train Epoch: 2 [500/600 (83%)]\tloss=0.2785\n",
      "2023-05-16T02:35:14Z INFO     Train Epoch: 2 [520/600 (87%)]\tloss=0.1561\n",
      "2023-05-16T02:35:58Z INFO     Train Epoch: 2 [540/600 (90%)]\tloss=0.0775\n",
      "2023-05-16T02:36:39Z INFO     Train Epoch: 2 [560/600 (93%)]\tloss=0.0639\n",
      "2023-05-16T02:37:14Z INFO     Train Epoch: 2 [580/600 (97%)]\tloss=0.2012\n",
      "2023-05-16T02:37:55Z INFO     Train Epoch: 2 [600/600 (100%)]\tloss=0.5424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 Elapsed time: 1402.1715151919052\n",
      "Epoch 2/3 Processed dataset length: 9600\n",
      "Epoch 2/3 Throughput:  6.8465\n",
      "Valid Accuracy: 95.0%\n",
      "\n",
      "[INFO]Finish all test\n",
      "[INFO]Average training time per epoch:  1387.8733\n",
      "[INFO]Average throughput per epoch:  6.9171\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "    args=[\"--epoch\", \"2\",\n",
    "         \"--log-interval\", \"20\",\n",
    "          \"--test-batch-size\", \"16\", \n",
    "          \"--batch-size\", \"16\",\n",
    "          \"--local-only\",\n",
    "          \"--dataset\", \"1\",\n",
    "          \"--model-path\", \"/ppml/model\"\n",
    "         ]\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8af46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin of the script\n",
      "Namespace(driver_port='29500', env=[['http_proxy', 'http://child-prc.intel.com:913/'], ['https_proxy', 'http://child-prc.intel.com:913/'], ['no_proxy', '10.239.45.10:8081,10.112.231.51,10.239.45.10,172.168.0.*'], ['GLOO_TCP_IFACE', 'ens803f0'], ['HF_DATASETS_OFFLINE', '1'], ['SGX_ENABLED', 'false']], image='10.239.45.10/arda/intelanalytics/bigdl-ppml-trusted-deep-learning-gramine-ref:2.3.0-SNAPSHOT', main_script='/ppml/test/pert.ipynb', main_script_args='--epochs 5 --log-interval 20 --test-batch-size 16 --batch-size 16 --local-only --dataset 1 --model-path /ppml/model', namespace='default', nnodes=2, pod_cpu='13', pod_epc_memory='68719476736', pod_memory='64G', volume=['{\"name\":\"device-plugin\", \"hostPath\":{\"path\":\"/var/lib/kubelet/device-plugins\"}}', '{\"name\":\"aesm-socket\", \"hostPath\":{\"path\":\"/var/run/aesmd/aesm.socket\"}}', '{\"name\":\"source-code\", \"nfs\":{\"server\":\"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/wangjian/idc\"}}', '{\"name\":\"nfs-data\", \"nfs\":{\"server\":\"172.168.0.205\", \"path\":\"/mnt/sdb/disk1/nfsdata/guancheng/hf\"}}', '{\"name\":\"nfs-model\", \"nfs\":{\"server\":\"172.168.0.205\", \"path\":\"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\"}}'], volume_mount=['{\"mountPath\":\"/var/lib/kubelet/device-plugins\",\"name\":\"device-plugin\"}', '{\"mountPath\":\"/var/run/aesmd/aesm.socket\",\"name\":\"aesm-socket\"}', '{\"mountPath\":\"/root/.cache\",\"name\":\"nfs-data\"}', '{\"mountPath\":\"/ppml/model\",\"name\":\"nfs-model\"}', '{\"mountPath\": \"/ppml/test\", \"name\":\"source-code\"}'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /ppml/test/pert.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ppml/test/pert.ipynb\n",
      "['python', '/ppml/test/pert.py', '--epochs', '5', '--log-interval', '20', '--test-batch-size', '16', '--batch-size', '16', '--local-only', '--dataset', '1', '--model-path', '/ppml/model']\n",
      "before service get created\n",
      "Created Driver Service: bigdl-idc-e7db535-driver-service\n",
      "service get created successfully\n",
      "Created Driver Pod: bigdl-idc-e7db535-driver\n",
      "Created Rank 1 Pod: bigdl-idc-e7db535-worker-1\n",
      "You can use the following commands to check out the pods status and logs.\n",
      "**** kubectl get pods -l bigdl-app=e7db535 ****\n",
      "**** kubectl logs bigdl-idc-e7db535-driver ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 14358 bytes to /ppml/test/pert.py\n"
     ]
    }
   ],
   "source": [
    "import k8s_deployment\n",
    "args = [\"--nnodes\", \"2\",\n",
    "        \"--namespace\", \"default\",\n",
    "        \"--image\", \"10.239.45.10/arda/intelanalytics/bigdl-ppml-trusted-deep-learning-gramine-ref:2.3.0-SNAPSHOT\",\n",
    "        \"--env\", \"http_proxy\", \"http://child-prc.intel.com:913/\",\n",
    "        \"--env\", \"https_proxy\", \"http://child-prc.intel.com:913/\",\n",
    "        \"--env\", \"no_proxy\", \"10.239.45.10:8081,10.112.231.51,10.239.45.10,172.168.0.*\",\n",
    "        \"--env\", \"GLOO_TCP_IFACE\", \"ens803f0\",\n",
    "        \"--env\", \"HF_DATASETS_OFFLINE\", \"1\",\n",
    "        \"--env\", \"SGX_ENABLED\", \"false\",\n",
    "        \"--pod_cpu\", \"13\",\n",
    "        \"--pod_memory\", \"64G\",\n",
    "        \"--pod_epc_memory\", \"68719476736\",\n",
    "        \"--driver_port\", \"29500\",\n",
    "        \"--volume\", '{\"name\":\"device-plugin\", \"hostPath\":{\"path\":\"/var/lib/kubelet/device-plugins\"}}',\n",
    "        \"--volume\", '{\"name\":\"aesm-socket\", \"hostPath\":{\"path\":\"/var/run/aesmd/aesm.socket\"}}',\n",
    "        \"--volume\", '{\"name\":\"source-code\", \"nfs\":{\"server\":\"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/wangjian/idc\"}}',\n",
    "        \"--volume\", '{\"name\":\"nfs-data\", \"nfs\":{\"server\":\"172.168.0.205\", \"path\":\"/mnt/sdb/disk1/nfsdata/guancheng/hf\"}}',\n",
    "        \"--volume\", '{\"name\":\"nfs-model\", \"nfs\":{\"server\":\"172.168.0.205\", \"path\":\"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\"}}',\n",
    "        \"--volume_mount\", '{\"mountPath\":\"/var/lib/kubelet/device-plugins\",\"name\":\"device-plugin\"}',\n",
    "        \"--volume_mount\", '{\"mountPath\":\"/var/run/aesmd/aesm.socket\",\"name\":\"aesm-socket\"}',\n",
    "        \"--volume_mount\", '{\"mountPath\":\"/root/.cache\",\"name\":\"nfs-data\"}',\n",
    "        \"--volume_mount\", '{\"mountPath\":\"/ppml/model\",\"name\":\"nfs-model\"}',\n",
    "        \"--volume_mount\", '{\"mountPath\": \"/ppml/test\", \"name\":\"source-code\"}',\n",
    "        \"--main_script\", \"/ppml/test/pert.ipynb\",\n",
    "        \"--main_script_args\", \"--epochs 5 --log-interval 20 --test-batch-size 16 --batch-size 16 --local-only --dataset 1 --model-path /ppml/model\"]\n",
    "k8s_deployment.run_k8s(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.define model\n",
    "2.define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. init ppml_context\n",
    "    k8s or local\n",
    "2. init model\n",
    "3. init dataset and dataloader\n",
    "4. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
