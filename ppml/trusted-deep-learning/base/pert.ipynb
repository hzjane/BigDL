{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e69a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from torch import nn\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distributed conf\n",
    "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "RANK = int(os.environ.get(\"RANK\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    print(123)\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch PERT Example\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16, metavar=\"N\",\n",
    "                        help=\"input batch size for training (default: 16)\")\n",
    "    parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
    "                        help=\"input batch size for testing (default: 1000)\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, metavar=\"N\",\n",
    "                        help=\"number of epochs to train (default: 10)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-5, metavar=\"LR\",\n",
    "                        help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
    "                        help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\"--dataset\", type=int, default=1, metavar=\"D\",\n",
    "                        help=\"dataset size (default 1 * 9600)\")\n",
    "    parser.add_argument(\"--save-model\", action=\"store_true\", default=False,\n",
    "                        help=\"For Saving the current Model\")\n",
    "    parser.add_argument(\"--local-only\", type=str, default=\"false\",\n",
    "                        help=\"If set to true, then load model from disk\")\n",
    "    parser.add_argument(\"--model-path\", type=str, default=\"/ppml/model\",\n",
    "                        help=\"Where to load model\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\",\n",
    "                    help=\"Where to train model, default is cpu\")\n",
    "    # Only for test purpose\n",
    "    parser.add_argument(\"--load-model\", action=\"store_true\", default=False,\n",
    "                        help=\"For loading the current model\")\n",
    "    parser.add_argument(\"--mini-batch\", type=int, default=0, metavar=\"M\",\n",
    "                    help=\"If set, the PyTorch will conduct M local-batch computation before doing a all_reduce sync\")\n",
    "    parser.add_argument(\"--log-interval\", type=int, default=2, metavar=\"N\",\n",
    "                    help=\"how many batches to wait before logging training status\")\n",
    "    parser.add_argument(\"--log-path\", type=str, default=\"\",\n",
    "                    help=\"Path to save logs. Print to StdOut if log-path is not set\")\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71182fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_log_path(args):\n",
    "    print(456)\n",
    "    if args.log_path == \"\":\n",
    "        logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        level=logging.DEBUG)\n",
    "    else:\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            level=logging.DEBUG,\n",
    "            filename=args.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # data_type is actually split, so that we can define dataset for train set/validate set\n",
    "    def __init__(self, data_type, dataset_load):\n",
    "        self.dataset_load = dataset_load\n",
    "        self.data = self.load_data(data_type)\n",
    "\n",
    "    def load_data(self, data_type):\n",
    "        tmp_dataset = load_dataset(path='seamew/ChnSentiCorp', split=data_type)\n",
    "        Data = {}\n",
    "        for i in range(self.dataset_load):\n",
    "            for idx, line in enumerate(tmp_dataset):\n",
    "                sample = line\n",
    "                Data[idx + i * len(tmp_dataset)] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(args, train_data, valid_data):\n",
    "    #init tokenizer\n",
    "    if args.local_only.lower() == \"true\":\n",
    "        checkpoint = args.model_path\n",
    "        tokenizer = BertTokenizer.from_pretrained(\n",
    "        checkpoint, model_max_length=512, local_files_only=True)\n",
    "    else:\n",
    "        checkpoint = 'hfl/chinese-pert-base'\n",
    "        tokenizer = BertTokenizer.from_pretrained(checkpoint, model_max_length=512)\n",
    "    if is_distributed():\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True, drop_last=False, seed=args.seed)\n",
    "        valid_sampler = DistributedSampler(\n",
    "            valid_data, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True, drop_last=False, seed=args.seed)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, collate_fn=lambda x: collate_fn(x, tokenizer), sampler=train_sampler)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_data, batch_size=args.test_batch_size, collate_fn=lambda x: collate_fn(x, tokenizer), sampler=valid_sampler)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_data, batch_size=args.test_batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "    return train_dataloader, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_distribute():\n",
    "    return dist.is_available() and WORLD_SIZE > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbb221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_distributed():\n",
    "    return dist.is_available() and dist.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e761643",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Define dataset, so it is easier to load different split in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ca11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a batch of data, which is used for training\n",
    "def collate_fn(batch_samples, tokenizer):\n",
    "    batch_text = []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_text.append(sample['text'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    # The tokenizer will make the data to be a good format for our model to understand\n",
    "    X = tokenizer(\n",
    "        batch_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8566cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        if args.local_only.lower() == \"true\":\n",
    "            checkpoint = args.model_path\n",
    "            self.bert_encoder = BertModel.from_pretrained(\n",
    "                checkpoint, local_files_only=True)\n",
    "        else:\n",
    "            checkpoint = 'hfl/chinese-pert-base'\n",
    "            self.bert_encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "def train_loop(args, dataloader, model, loss_fn, optimizer, epoch, total_loss):\n",
    "    # Set to train mode\n",
    "    model.train()\n",
    "    total_dataset = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    enumerator = enumerate(dataloader, start=1)\n",
    "    for batch, (X, y) in enumerator:\n",
    "        my_context = model.no_sync if WORLD_SIZE > 1 and args.mini_batch > 0 and batch % args.mini_batch != 0 else nullcontext\n",
    "        with my_context():\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            # Forward pass\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "        if args.mini_batch == 0 or batch % args.mini_batch == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_dataset += args.batch_size\n",
    "        from torch.utils.tensorboard import SummaryWriter   \n",
    "        writer = SummaryWriter('/ppml/test/pert.log')\n",
    "        writer.add_scalar('loss',loss.item(), (epoch - 1) * len(dataloader) + batch)\n",
    "        if batch % args.log_interval == 0:\n",
    "            msg = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
    "                epoch, batch, len(dataloader),\n",
    "                100. * batch / len(dataloader), loss.item())\n",
    "            logging.info(msg)\n",
    "\n",
    "    return total_loss, total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aecfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test loop to get acc\n",
    "def test_loop(args, dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    #correct /= size\n",
    "    #correct *= WORLD_SIZE\n",
    "    correct = correct / (size / WORLD_SIZE)\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(args, train_dataloader, valid_dataloader, model, loss_fn, optimizer):\n",
    "    total_loss = 0.\n",
    "    best_acc = 0.\n",
    "    total_time = 0.\n",
    "    total_throughput = 0.\n",
    "\n",
    "    for t in range(args.epochs):\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1}\\n-------------------------------\")\n",
    "#         if is_distributed():\n",
    "#             # set seed\n",
    "#             train_dataloader.sampler.set_epoch(t)\n",
    "#             valid_dataloader.sampler.set_epoch(t)\n",
    "        start = time.perf_counter()\n",
    "        # start to train\n",
    "        total_loss, total_dataset = train_loop(\n",
    "            args, train_dataloader, model, loss_fn, optimizer, t+1, total_loss)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1} Elapsed time:\",\n",
    "              end - start, flush=True)\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1} Processed dataset length:\",\n",
    "              total_dataset, flush=True)\n",
    "        msg = \"Epoch {}/{} Throughput: {: .4f}\".format(\n",
    "            t+1, args.epochs+1, 1.0 * total_dataset / (end-start))\n",
    "        total_time += (end - start)\n",
    "        total_throughput += total_dataset\n",
    "        print(msg, flush=True)\n",
    "        # to valid acc\n",
    "        valid_acc = test_loop(args, valid_dataloader, model, mode='Valid')\n",
    "\n",
    "    print(\"[INFO]Finish all test\", flush=True)\n",
    "    msg = \"[INFO]Average training time per epoch: {: .4f}\".format(total_time / args.epochs)\n",
    "    print(msg, flush=True)\n",
    "\n",
    "    msg = \"[INFO]Average throughput per epoch: {: .4f}\".format(total_throughput / total_time)\n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=None):\n",
    "    # parse args\n",
    "    args = parse_args(args)\n",
    "    print(args)\n",
    "    # set log_path\n",
    "    set_log_path(args)\n",
    "    \n",
    "    # init data_loader\n",
    "    torch.manual_seed(args.seed) # set seed\n",
    "    \n",
    "    # init pytorch distributed network if need\n",
    "    if should_distribute():\n",
    "        print(\"Using distributed PyTorch with {} backend\".format(\n",
    "            \"GLOO\"), flush=True)\n",
    "        dist.init_process_group(backend=dist.Backend.GLOO)\n",
    "\n",
    "    # Load train and valid data\n",
    "    print(\"[INFO]Before data get loaded\", flush=True)\n",
    "    train_data = Dataset('train', args.dataset)\n",
    "    print(\"######train data length:\", len(train_data.data), flush=True)\n",
    "    valid_data = Dataset('validation', 1)\n",
    "\n",
    "    train_dataloader, valid_dataloader = get_dataloader(args, train_data, valid_data)\n",
    "    print(\"[INFO]Data get loaded successfully\", flush=True)\n",
    "\n",
    "    #init model\n",
    "    model = NeuralNetwork(args).to(args.device)\n",
    "    print(\"what happen\")\n",
    "    # load pre-train model\n",
    "    if (args.load_model):\n",
    "        model.load_state_dict(torch.load('./pert.bin'))\n",
    "    # local or distributed model\n",
    "    if is_distributed():\n",
    "        Distributor = nn.parallel.DistributedDataParallel\n",
    "        model = Distributor(model, find_unused_parameters=True)\n",
    "    \n",
    "    # set loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    # train epoch\n",
    "    save_model = do_train(args, train_dataloader, valid_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    # save model and exit\n",
    "    if (args.save_model):\n",
    "        torch.save(save_model.state_dict(), \"pert.bin\")\n",
    "    if is_distributed():\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "    import ppml_conf\n",
    "    local_conf = ppml_conf.PPMLConf(k8s_enabled = False) \\\n",
    "    .set(\"epoch\", \"2\") \\\n",
    "    .set(\"log-interval\", \"20\") \\\n",
    "    .set(\"test-batch-size\", \"16\") \\\n",
    "    .set(\"batch-size\", \"16\") \\\n",
    "    .set(\"local-only\", \"true\") \\\n",
    "    .set(\"dataset\", \"1\") \\\n",
    "    .set(\"model-path\", \"/ppml/model\")\n",
    "\n",
    "    args1=local_conf.conf_to_args()\n",
    "        \n",
    "#     args=[\"--epoch\", \"2\",\n",
    "#          \"--log-interval\", \"20\",\n",
    "#           \"--test-batch-size\", \"16\", \n",
    "#           \"--batch-size\", \"16\",\n",
    "#           \"--local-only\", \"true\",\n",
    "#           \"--dataset\", \"1\",\n",
    "#           \"--model-path\", \"/ppml/model\"\n",
    "#          ]\n",
    "#     print(args)\n",
    "    main(args1)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8af46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import k8s_deployment\n",
    "\n",
    "import ppml_conf\n",
    "k8s_conf = ppml_conf.PPMLConf(k8s_enabled = True, sgx_enabled=False) \\\n",
    ".set_k8s_env(\"GLOO_TCP_IFACE\", \"ens803f0\") \\\n",
    ".set_k8s_env(\"HF_DATASETS_OFFLINE\", \"1\") \\\n",
    ".set_k8s(\"nnodes\", \"2\") \\\n",
    ".set_k8s(\"pod_cpu\", \"13\") \\\n",
    ".set_k8s(\"pod_memory\", \"64G\") \\\n",
    ".set_k8s(\"pod_epc_memory\", \"68719476736\")\n",
    "\n",
    "# set_volumn_host(\"volume_name,host_path\")\n",
    "# set_volumn_nfs(\"volume_name, nfs_server, nfs_path\")\n",
    "# set_volumn_mount(\"mount_path,volume_name\")\n",
    "k8s_conf \\\n",
    ".set_volume_nfs(\"source-code\", \"172.168.0.205\",\"/mnt/sdb/disk1/nfsdata/wangjian/idc\") \\\n",
    ".set_volume_mount(\"/ppml/notebook/nfs\", \"source-code\") \\\n",
    ".set_volume_nfs(\"nfs-data\", \"172.168.0.205\", \"/mnt/sdb/disk1/nfsdata/guancheng/hf\") \\\n",
    ".set_volume_mount(\"/root/.cache\", \"nfs-data\") \\\n",
    ".set_volume_nfs(\"nfs-model\", \"172.168.0.205\", \"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\") \\\n",
    ".set_volume_mount(\"/ppml/model\", \"nfs-model\") \\\n",
    "\n",
    "k8s_args = k8s_conf.conf_to_args()\n",
    "\n",
    "\n",
    "k8s_deployment.run_k8s(k8s_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.define model\n",
    "# 2.define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. init ppml_context\n",
    "#     k8s or local\n",
    "# 2. init model\n",
    "# 3. init dataset and dataloader\n",
    "# 4. \n",
    "# import ppml_conf\n",
    "# myconf = ppml_conf.PPMLConf()\n",
    "# myconf.test()\n",
    "# myconf.conf_to_args()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
