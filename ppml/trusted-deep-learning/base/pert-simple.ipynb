{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from torch import nn\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import BertTokenizer, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get local or distributed conf\n",
    "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "RANK = int(os.environ.get(\"RANK\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98710c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch PERT Example\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16, metavar=\"N\",\n",
    "                        help=\"input batch size for training (default: 16)\")\n",
    "    parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
    "                        help=\"input batch size for testing (default: 1000)\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, metavar=\"N\",\n",
    "                        help=\"number of epochs to train (default: 10)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-5, metavar=\"LR\",\n",
    "                        help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
    "                        help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\"--dataset-size\", type=int, default=1, metavar=\"D\",\n",
    "                        help=\"dataset size (default 1 * 9600)\")\n",
    "    parser.add_argument(\"--model-save-path\", type=str, default=\"\",\n",
    "                        help=\"For Saving the current Model\")\n",
    "    parser.add_argument(\"--model-load-path\", type=str, default=\"\",\n",
    "                        help=\"Where to load pretrained model, can set it to /ppml/model\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\",\n",
    "                    help=\"Where to train model, default is cpu\")\n",
    "    # Only for test purpose\n",
    "    parser.add_argument(\"--load-current-model\", type=str, default=\"\",\n",
    "                        help=\"For loading the current model\")\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b866e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # data_type is actually split, so that we can define dataset for train set/validate set\n",
    "    def __init__(self, data_type, dataset_load_iter):\n",
    "        self.dataset_load_iter = dataset_load_iter\n",
    "        self.data = self.load_data(data_type)\n",
    "\n",
    "    def load_data(self, data_type):\n",
    "        tmp_dataset = load_dataset(path='seamew/ChnSentiCorp', split=data_type)\n",
    "        Data = {}\n",
    "        for i in range(self.dataset_load_iter):\n",
    "            for idx, line in enumerate(tmp_dataset):\n",
    "                sample = line\n",
    "                Data[idx + i * len(tmp_dataset)] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a238a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a batch of data, which is used for training\n",
    "def collate_fn(batch_samples, tokenizer):\n",
    "    batch_text = []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_text.append(sample['text'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    # The tokenizer will make the data to be a good format for our model to understand\n",
    "    X = tokenizer(\n",
    "        batch_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2489abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(tokenizer_load_path, batch_size, test_batch_size, train_data, valid_data):\n",
    "    #init tokenizer\n",
    "    if tokenizer_load_path != \"\":\n",
    "        checkpoint = tokenizer_load_path\n",
    "        tokenizer = BertTokenizer.from_pretrained(\n",
    "        checkpoint, model_max_length=512, local_files_only=True)\n",
    "    else:\n",
    "        checkpoint = 'hfl/chinese-pert-base'\n",
    "        tokenizer = BertTokenizer.from_pretrained(checkpoint, model_max_length=512)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_data, batch_size=test_batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "    return train_dataloader, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        if args.model_load_path != \"\":\n",
    "            checkpoint = args.model_load_path\n",
    "            self.bert_encoder = BertModel.from_pretrained(\n",
    "                checkpoint, local_files_only=True)\n",
    "        else:\n",
    "            checkpoint = 'hfl/chinese-pert-base'\n",
    "            self.bert_encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "def train_loop(device, dataloader, model, loss_fn, optimizer, epoch, total_loss):\n",
    "    # Set to train mode\n",
    "    model.train()\n",
    "    total_dataset = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    enumerator = enumerate(dataloader, start=1)\n",
    "    for batch, (X, y) in enumerator:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_dataset += dataloader.batch_size\n",
    "        if batch % 1 == 0:\n",
    "            msg = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
    "                epoch, batch, len(dataloader),\n",
    "                100. * batch / len(dataloader), loss.item())\n",
    "            print(msg, flush=True)\n",
    "\n",
    "    return total_loss, total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test loop to get acc\n",
    "def test_loop(device, dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    #correct /= size\n",
    "    #correct *= WORLD_SIZE\n",
    "    correct = correct / (size / WORLD_SIZE)\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb763da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(epochs, device, train_dataloader, valid_dataloader, model, loss_fn, optimizer):\n",
    "    total_loss = 0.\n",
    "    best_acc = 0.\n",
    "    total_time = 0.\n",
    "    total_throughput = 0.\n",
    "\n",
    "    for t in range(epochs):\n",
    "#         print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\", flush=True)\n",
    "        start = time.perf_counter()\n",
    "        # start to train\n",
    "        total_loss, total_dataset = train_loop(\n",
    "            device, train_dataloader, model, loss_fn, optimizer, t+1, total_loss)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Epoch {t+1}/{epochs} Elapsed time:\",\n",
    "              end - start, flush=True)\n",
    "        print(f\"Epoch {t+1}/{epochs} Processed dataset length:\",\n",
    "              total_dataset, flush=True)\n",
    "        msg = \"Epoch {}/{} Throughput: {: .4f}\".format(\n",
    "            t+1, epochs+1, 1.0 * total_dataset / (end-start))\n",
    "        total_time += (end - start)\n",
    "        total_throughput += total_dataset\n",
    "        print(msg, flush=True)\n",
    "        # to valid acc\n",
    "        valid_acc = test_loop(device, valid_dataloader, model, mode='Valid')\n",
    "\n",
    "    print(\"[INFO]Finish all test\", flush=True)\n",
    "    msg = \"[INFO]Average training time per epoch: {: .4f}\".format(total_time / epochs)\n",
    "    print(msg, flush=True)\n",
    "\n",
    "    msg = \"[INFO]Average throughput per epoch: {: .4f}\".format(total_throughput / total_time)\n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec708004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=None):\n",
    "    # parse args\n",
    "    args = parse_args(args)\n",
    "    print(args)\n",
    "    torch.manual_seed(args.seed) # set seed\n",
    "\n",
    "    # Load train and valid data and init data_loader\n",
    "    train_data = Dataset('train', args.dataset_size)\n",
    "    print(\"######train data length:\", len(train_data.data), flush=True)\n",
    "    valid_data = Dataset('validation', 1)\n",
    "\n",
    "    train_dataloader, valid_dataloader = get_dataloader(args.model_load_path, args.batch_size, args.test_batch_size, train_data, valid_data)\n",
    "    print(\"[INFO]Data get loaded successfully\", flush=True)\n",
    "\n",
    "    #init model\n",
    "    model = NeuralNetwork(args).to(args.device)\n",
    "\n",
    "    # load pre-train model\n",
    "    if args.load_current_model != \"\":\n",
    "        model.load_state_dict(torch.load(args.load_current_model))\n",
    "    \n",
    "    # set loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    # get distributed\n",
    "#     model, train_dataloader, valid_dataloader = ppml_context.get_distributed(model, train_dataloader, valid_dataloader)\n",
    "    import ppml_context\n",
    "    mycontext = ppml_context.PPMLContext(\"local\")\n",
    "    model, train_dataloader, valid_dataloader = mycontext.set_training(model, train_dataloader, valid_dataloader)\n",
    "    # train epoch\n",
    "    save_model = do_train(args.epochs, args.device, train_dataloader, valid_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    # save model and exit\n",
    "    if args.model_save_path != \"\":\n",
    "        torch.save(save_model.state_dict(), args.model_save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import sys\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "    import ppml_context\n",
    "    local_conf = ppml_context.PPMLConf(k8s_enabled = False) \\\n",
    "    .set(\"epoch\", \"2\") \\\n",
    "    .set(\"test-batch-size\", \"16\") \\\n",
    "    .set(\"batch-size\", \"16\") \\\n",
    "    .set(\"dataset-size\", \"1\") \\\n",
    "    .set(\"model-load-path\", \"/ppml/model\")\n",
    "\n",
    "    args1=local_conf.conf_to_args()\n",
    "        \n",
    "    main(args1)\n",
    "    local_conf.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7434a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppml_context\n",
    "k8s_conf = ppml_context.PPMLConf(k8s_enabled = True, sgx_enabled=False) \\\n",
    ".set(\"bigdl.ppml.k8s.env.GLOO_TCP_IFACE\", \"ens803f0\") \\\n",
    ".set(\"bigdl.ppml.k8s.env.HF_DATASETS_OFFLINE\", \"1\") \\\n",
    ".set(\"bigdl.ppml.k8s.conf.nnodes\", \"2\") \\\n",
    ".set(\"bigdl.ppml.k8s.conf.pod_cpu\", \"13\") \\\n",
    ".set(\"bigdl.ppml.k8s.conf.pod_memory\", \"64G\") \\\n",
    ".set(\"bigdl.ppml.k8s.conf.pod_epc_memory\", \"68719476736\") \\\n",
    ".set(\"main_script\", \"pert-simple\")\n",
    "\n",
    "# set(\"bigdl.ppml.k8s.volume_nfs.volume_name,host_path\")\n",
    "# set(\"bigdl.ppml.k8s.volume_nfs.volume_name, nfs_server, nfs_path\")\n",
    "# set(\"bigdl.ppml.k8s.volume_mount.mount_path,volume_name\")\n",
    "k8s_conf \\\n",
    ".set(\"bigdl.ppml.k8s.volume_nfs.source-code\", \"172.168.0.205\",\"/mnt/sdb/disk1/nfsdata/wangjian/idc\") \\\n",
    ".set(\"bigdl.ppml.k8s.volume_mount.source-code\", \"/ppml/notebook/nfs\") \\\n",
    ".set(\"bigdl.ppml.k8s.volume_nfs.nfs-data\", \"172.168.0.205\", \"/mnt/sdb/disk1/nfsdata/guancheng/hf\") \\\n",
    ".set(\"bigdl.ppml.k8s.volume_mount.nfs-data\", \"/root/.cache\") \\\n",
    ".set(\"bigdl.ppml.k8s.volume_nfs.nfs-model\", \"172.168.0.205\", \"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\") \\\n",
    ".set(\"bigdl.ppml.k8s.volume_mount.nfs-model\", \"/ppml/model\") \\\n",
    "\n",
    "k8s_conf.run_k8s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b197ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb0963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
