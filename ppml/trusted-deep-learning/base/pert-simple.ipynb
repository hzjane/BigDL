{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa6ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from torch import nn\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import BertTokenizer, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e2543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get local or distributed conf\n",
    "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "RANK = int(os.environ.get(\"RANK\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98710c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch PERT Example\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16, metavar=\"N\",\n",
    "                        help=\"input batch size for training (default: 16)\")\n",
    "    parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
    "                        help=\"input batch size for testing (default: 1000)\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, metavar=\"N\",\n",
    "                        help=\"number of epochs to train (default: 10)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-5, metavar=\"LR\",\n",
    "                        help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
    "                        help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\"--dataset-size\", type=int, default=1, metavar=\"D\",\n",
    "                        help=\"dataset size (default 1 * 9600)\")\n",
    "    parser.add_argument(\"--model-save-path\", type=str, default=\"\",\n",
    "                        help=\"For Saving the current Model\")\n",
    "    parser.add_argument(\"--model-load-path\", type=str, default=\"\",\n",
    "                        help=\"Where to load pretrained model, can set it to /ppml/model\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\",\n",
    "                    help=\"Where to train model, default is cpu\")\n",
    "    # Only for test purpose\n",
    "    parser.add_argument(\"--load-current-model\", type=str, default=\"\",\n",
    "                        help=\"For loading the current model\")\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b866e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # data_type is actually split, so that we can define dataset for train set/validate set\n",
    "    def __init__(self, data_type, dataset_load_iter):\n",
    "        self.dataset_load_iter = dataset_load_iter\n",
    "        self.data = self.load_data(data_type)\n",
    "\n",
    "    def load_data(self, data_type):\n",
    "        tmp_dataset = load_dataset(path='seamew/ChnSentiCorp', split=data_type)\n",
    "        Data = {}\n",
    "        for i in range(self.dataset_load_iter):\n",
    "            for idx, line in enumerate(tmp_dataset):\n",
    "                sample = line\n",
    "                Data[idx + i * len(tmp_dataset)] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a238a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a batch of data, which is used for training\n",
    "def collate_fn(batch_samples, tokenizer):\n",
    "    batch_text = []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_text.append(sample['text'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    # The tokenizer will make the data to be a good format for our model to understand\n",
    "    X = tokenizer(\n",
    "        batch_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2489abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(tokenizer_load_path, batch_size, test_batch_size, train_data, valid_data):\n",
    "    #init tokenizer\n",
    "    if tokenizer_load_path != \"\":\n",
    "        checkpoint = tokenizer_load_path\n",
    "        tokenizer = BertTokenizer.from_pretrained(\n",
    "        checkpoint, model_max_length=512, local_files_only=True)\n",
    "    else:\n",
    "        checkpoint = 'hfl/chinese-pert-base'\n",
    "        tokenizer = BertTokenizer.from_pretrained(checkpoint, model_max_length=512)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_data, batch_size=test_batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer))\n",
    "    return train_dataloader, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440d76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        if args.model_load_path != \"\":\n",
    "            checkpoint = args.model_load_path\n",
    "            self.bert_encoder = BertModel.from_pretrained(\n",
    "                checkpoint, local_files_only=True)\n",
    "        else:\n",
    "            checkpoint = 'hfl/chinese-pert-base'\n",
    "            self.bert_encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011f6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "def train_loop(device, dataloader, model, loss_fn, optimizer, epoch, total_loss):\n",
    "    # Set to train mode\n",
    "    model.train()\n",
    "    total_dataset = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    enumerator = enumerate(dataloader, start=1)\n",
    "    for batch, (X, y) in enumerator:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_dataset += dataloader.batch_size\n",
    "        if batch % 1 == 0:\n",
    "            msg = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
    "                epoch, batch, len(dataloader),\n",
    "                100. * batch / len(dataloader), loss.item())\n",
    "            print(msg, flush=True)\n",
    "\n",
    "    return total_loss, total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0883093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test loop to get acc\n",
    "def test_loop(device, dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    #correct /= size\n",
    "    #correct *= WORLD_SIZE\n",
    "    correct = correct / (size / WORLD_SIZE)\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb763da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(epochs, device, train_dataloader, valid_dataloader, model, loss_fn, optimizer):\n",
    "    total_loss = 0.\n",
    "    best_acc = 0.\n",
    "    total_time = 0.\n",
    "    total_throughput = 0.\n",
    "\n",
    "    for t in range(epochs):\n",
    "#         print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\", flush=True)\n",
    "        start = time.perf_counter()\n",
    "        # start to train\n",
    "        total_loss, total_dataset = train_loop(\n",
    "            device, train_dataloader, model, loss_fn, optimizer, t+1, total_loss)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Epoch {t+1}/{epochs} Elapsed time:\",\n",
    "              end - start, flush=True)\n",
    "        print(f\"Epoch {t+1}/{epochs} Processed dataset length:\",\n",
    "              total_dataset, flush=True)\n",
    "        msg = \"Epoch {}/{} Throughput: {: .4f}\".format(\n",
    "            t+1, epochs+1, 1.0 * total_dataset / (end-start))\n",
    "        total_time += (end - start)\n",
    "        total_throughput += total_dataset\n",
    "        print(msg, flush=True)\n",
    "        # to valid acc\n",
    "        valid_acc = test_loop(device, valid_dataloader, model, mode='Valid')\n",
    "\n",
    "    print(\"[INFO]Finish all test\", flush=True)\n",
    "    msg = \"[INFO]Average training time per epoch: {: .4f}\".format(total_time / epochs)\n",
    "    print(msg, flush=True)\n",
    "\n",
    "    msg = \"[INFO]Average throughput per epoch: {: .4f}\".format(total_throughput / total_time)\n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec708004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=None):\n",
    "    # parse args\n",
    "    args = parse_args(args)\n",
    "    print(args)\n",
    "    torch.manual_seed(args.seed) # set seed\n",
    "    \n",
    "    #init\n",
    "    import ppml_context\n",
    "#     ppml_context = ppml_context.PPMLContext(k8s_enabled=True)\n",
    "\n",
    "    # Load train and valid data and init data_loader\n",
    "    train_data = Dataset('train', args.dataset_size)\n",
    "    print(\"######train data length:\", len(train_data.data), flush=True)\n",
    "    valid_data = Dataset('validation', 1)\n",
    "\n",
    "    train_dataloader, valid_dataloader = get_dataloader(args.model_load_path, args.batch_size, args.test_batch_size, train_data, valid_data)\n",
    "    print(\"[INFO]Data get loaded successfully\", flush=True)\n",
    "\n",
    "    #init model\n",
    "    model = NeuralNetwork(args).to(args.device)\n",
    "\n",
    "    # load pre-train model\n",
    "    if args.load_current_model != \"\":\n",
    "        model.load_state_dict(torch.load(args.load_current_model))\n",
    "    \n",
    "    # set loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    # get distributed\n",
    "#     model, train_dataloader, valid_dataloader = ppml_context.get_distributed(model, train_dataloader, valid_dataloader)\n",
    "    \n",
    "    # train epoch\n",
    "    save_model = do_train(args.epochs, args.device, train_dataloader, valid_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    # save model and exit\n",
    "    if args.model_save_path != \"\":\n",
    "        torch.save(save_model.state_dict(), args.model_save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f0c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "['--epoch', '2', '--test-batch-size', '16', '--batch-size', '16', '--dataset-size', '1', '--model-load-path', '/ppml/model']\n",
      "Namespace(batch_size=16, test_batch_size=16, epochs=2, lr=1e-05, seed=1, dataset_size=1, model_save_path='', model_load_path='/ppml/model', device='cpu', load_current_model='')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (/root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######train data length: 9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset chn_senti_corp (/root/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Data get loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /ppml/model were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1/600 (0%)]\tloss=0.6923\n",
      "Train Epoch: 1 [2/600 (0%)]\tloss=0.6928\n",
      "Train Epoch: 1 [3/600 (0%)]\tloss=0.6939\n",
      "Train Epoch: 1 [4/600 (1%)]\tloss=0.6918\n",
      "Train Epoch: 1 [5/600 (1%)]\tloss=0.6887\n",
      "Train Epoch: 1 [6/600 (1%)]\tloss=0.6881\n",
      "Train Epoch: 1 [7/600 (1%)]\tloss=0.6918\n",
      "Train Epoch: 1 [8/600 (1%)]\tloss=0.6880\n",
      "Train Epoch: 1 [9/600 (2%)]\tloss=0.6916\n",
      "Train Epoch: 1 [10/600 (2%)]\tloss=0.6813\n",
      "Train Epoch: 1 [11/600 (2%)]\tloss=0.6892\n",
      "Train Epoch: 1 [12/600 (2%)]\tloss=0.6914\n",
      "Train Epoch: 1 [13/600 (2%)]\tloss=0.6909\n",
      "Train Epoch: 1 [14/600 (2%)]\tloss=0.6877\n",
      "Train Epoch: 1 [15/600 (2%)]\tloss=0.6981\n",
      "Train Epoch: 1 [16/600 (3%)]\tloss=0.6847\n",
      "Train Epoch: 1 [17/600 (3%)]\tloss=0.6822\n",
      "Train Epoch: 1 [18/600 (3%)]\tloss=0.6859\n",
      "Train Epoch: 1 [19/600 (3%)]\tloss=0.6845\n",
      "Train Epoch: 1 [20/600 (3%)]\tloss=0.6981\n",
      "Train Epoch: 1 [21/600 (4%)]\tloss=0.6841\n",
      "Train Epoch: 1 [22/600 (4%)]\tloss=0.6907\n",
      "Train Epoch: 1 [23/600 (4%)]\tloss=0.6871\n",
      "Train Epoch: 1 [24/600 (4%)]\tloss=0.6884\n",
      "Train Epoch: 1 [25/600 (4%)]\tloss=0.6779\n",
      "Train Epoch: 1 [26/600 (4%)]\tloss=0.6693\n",
      "Train Epoch: 1 [27/600 (4%)]\tloss=0.6811\n",
      "Train Epoch: 1 [28/600 (5%)]\tloss=0.6692\n",
      "Train Epoch: 1 [29/600 (5%)]\tloss=0.6863\n",
      "Train Epoch: 1 [30/600 (5%)]\tloss=0.6978\n",
      "Train Epoch: 1 [31/600 (5%)]\tloss=0.6697\n",
      "Train Epoch: 1 [32/600 (5%)]\tloss=0.6762\n",
      "Train Epoch: 1 [33/600 (6%)]\tloss=0.6623\n",
      "Train Epoch: 1 [34/600 (6%)]\tloss=0.6736\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import sys\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "    import ppml_conf\n",
    "    local_conf = ppml_conf.PPMLConf(k8s_enabled = False) \\\n",
    "    .set(\"epoch\", \"2\") \\\n",
    "    .set(\"test-batch-size\", \"16\") \\\n",
    "    .set(\"batch-size\", \"16\") \\\n",
    "    .set(\"dataset-size\", \"1\") \\\n",
    "    .set(\"model-load-path\", \"/ppml/model\")\n",
    "\n",
    "    args1=local_conf.conf_to_args()\n",
    "        \n",
    "    main(args1)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7434a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "['--namespace', 'default', '--image', 'intelanalytics/bigdl-ppml-trusted-deep-learning-gramine-ref:2.4.0-SNAPSHOT', '--driver_port', '29500', '--nnodes', '2', '--pod_cpu', '13', '--pod_memory', '64G', '--pod_epc_memory', '68719476736', '--env', 'http_proxy', 'http://child-prc.intel.com:913/', '--env', 'https_proxy', 'http://child-prc.intel.com:913/', '--env', 'no_proxy', '10.239.45.10:8081,10.112.231.51,10.239.45.10,172.168.0.*', '--env', 'SGX_ENABLED', 'false', '--env', 'GLOO_TCP_IFACE', 'ens803f0', '--env', 'HF_DATASETS_OFFLINE', '1', '--volume', '{\"name\": \"device-plugin\", \"hostPath\": {\"path\": \"/var/lib/kubelet/device-plugins\"}}', '--volume', '{\"name\": \"aesm-socket\", \"hostPath\": {\"path\": \"/var/run/aesmd/aesm.socket\"}}', '--volume', '{\"name\": \"source-code\", \"nfs\": {\"server\": \"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/wangjian/idc\"}}', '--volume', '{\"name\": \"nfs-data\", \"nfs\": {\"server\": \"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/guancheng/hf\"}}', '--volume', '{\"name\": \"nfs-model\", \"nfs\": {\"server\": \"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\"}}', '--volume_mount', '{\"mountPath\": \"/var/lib/kubelet/device-plugins\", \"name\": \"device-plugin\"}', '--volume_mount', '{\"mountPath\": \"/var/run/aesmd/aesm.socket\", \"name\": \"aesm-socket\"}', '--volume_mount', '{\"mountPath\": \"/ppml/notebook/nfs\", \"name\": \"source-code\"}', '--volume_mount', '{\"mountPath\": \"/root/.cache\", \"name\": \"nfs-data\"}', '--volume_mount', '{\"mountPath\": \"/ppml/model\", \"name\": \"nfs-model\"}', '--main_script', '/ppml/notebook/nfs/pert-simple.py.ipynb', '--main_script_args', '--test 2']\n",
      "begin of the script\n",
      "Namespace(nnodes=2, image='intelanalytics/bigdl-ppml-trusted-deep-learning-gramine-ref:2.4.0-SNAPSHOT', driver_port='29500', namespace='default', env=[['http_proxy', 'http://child-prc.intel.com:913/'], ['https_proxy', 'http://child-prc.intel.com:913/'], ['no_proxy', '10.239.45.10:8081,10.112.231.51,10.239.45.10,172.168.0.*'], ['SGX_ENABLED', 'false'], ['GLOO_TCP_IFACE', 'ens803f0'], ['HF_DATASETS_OFFLINE', '1']], pod_cpu='13', pod_epc_memory='68719476736', pod_memory='64G', volume=['{\"name\": \"device-plugin\", \"hostPath\": {\"path\": \"/var/lib/kubelet/device-plugins\"}}', '{\"name\": \"aesm-socket\", \"hostPath\": {\"path\": \"/var/run/aesmd/aesm.socket\"}}', '{\"name\": \"source-code\", \"nfs\": {\"server\": \"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/wangjian/idc\"}}', '{\"name\": \"nfs-data\", \"nfs\": {\"server\": \"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/guancheng/hf\"}}', '{\"name\": \"nfs-model\", \"nfs\": {\"server\": \"172.168.0.205\", \"path\": \"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\"}}'], volume_mount=['{\"mountPath\": \"/var/lib/kubelet/device-plugins\", \"name\": \"device-plugin\"}', '{\"mountPath\": \"/var/run/aesmd/aesm.socket\", \"name\": \"aesm-socket\"}', '{\"mountPath\": \"/ppml/notebook/nfs\", \"name\": \"source-code\"}', '{\"mountPath\": \"/root/.cache\", \"name\": \"nfs-data\"}', '{\"mountPath\": \"/ppml/model\", \"name\": \"nfs-model\"}'], main_script='/ppml/notebook/nfs/pert-simple.py.ipynb', main_script_args='--test 2')\n",
      "/ppml/notebook/nfs/pert-simple.py.ipynb\n",
      "['python', '/ppml/notebook/nfs/pert-simple.py.py', '--test', '2']\n",
      "before service get created\n",
      "Created Driver Service: bigdl-idc-8a2f3b2-driver-service\n",
      "service get created successfully\n",
      "Created Driver Pod: bigdl-idc-8a2f3b2-driver\n",
      "Created Rank 1 Pod: bigdl-idc-8a2f3b2-worker-1\n",
      "You can use the following commands to check out the pods status and logs.\n",
      "**** kubectl get pods -l bigdl-app=8a2f3b2 ****\n",
      "**** kubectl logs bigdl-idc-8a2f3b2-driver ****\n"
     ]
    }
   ],
   "source": [
    "import k8s_deployment\n",
    "\n",
    "import ppml_conf\n",
    "k8s_conf = ppml_conf.PPMLConf(k8s_enabled = True, sgx_enabled=False) \\\n",
    ".set_k8s_env(\"GLOO_TCP_IFACE\", \"ens803f0\") \\\n",
    ".set_k8s_env(\"HF_DATASETS_OFFLINE\", \"1\") \\\n",
    ".set_k8s(\"nnodes\", \"2\") \\\n",
    ".set_k8s(\"pod_cpu\", \"13\") \\\n",
    ".set_k8s(\"pod_memory\", \"64G\") \\\n",
    ".set_k8s(\"pod_epc_memory\", \"68719476736\")\n",
    "\n",
    "# set_volumn_host(\"volume_name,host_path\")\n",
    "# set_volumn_nfs(\"volume_name, nfs_server, nfs_path\")\n",
    "# set_volumn_mount(\"mount_path,volume_name\")\n",
    "k8s_conf \\\n",
    ".set_volume_nfs(\"source-code\", \"172.168.0.205\",\"/mnt/sdb/disk1/nfsdata/wangjian/idc\") \\\n",
    ".set_volume_mount(\"/ppml/notebook/nfs\", \"source-code\") \\\n",
    ".set_volume_nfs(\"nfs-data\", \"172.168.0.205\", \"/mnt/sdb/disk1/nfsdata/guancheng/hf\") \\\n",
    ".set_volume_mount(\"/root/.cache\", \"nfs-data\") \\\n",
    ".set_volume_nfs(\"nfs-model\", \"172.168.0.205\", \"/mnt/sdb/disk1/nfsdata/guancheng/model/chinese-pert-base\") \\\n",
    ".set_volume_mount(\"/ppml/model\", \"nfs-model\") \\\n",
    "\n",
    "k8s_args = k8s_conf.conf_to_args()\n",
    "\n",
    "\n",
    "k8s_deployment.run_k8s(k8s_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b197ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb0963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
